{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gusya-soc/notebook_collection/blob/main/transformer_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsKZwF6Iv-wA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDGJ-WDopetF"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOplcb8wwG_I"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZxtD1oHwN2W"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import models,layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOuGoSvgwYHS"
      },
      "outputs": [],
      "source": [
        "# @tf.keras.utils.register_keras_serializable()\n",
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self,head_num,hidden_size):\n",
        "        super().__init__()\n",
        "        self.head_num = head_num\n",
        "        self.hidden_size = hidden_size\n",
        "        self.sub_hidden = hidden_size // head_num\n",
        "        self.dense_q = layers.Dense(hidden_size)\n",
        "        self.dense_k = layers.Dense(hidden_size)\n",
        "        self.dense_v = layers.Dense(hidden_size)\n",
        "        self.dense_output = layers.Dense(hidden_size)\n",
        "\n",
        "    def scale_dot_product(self,q,k,v,mask):     #q,k,v shape (batch_size,head_num,seq_size_,hidden_size)\n",
        "        matmul_qk =  tf.matmul(q,k,transpose_b=True)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scale_qk = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "        if mask is not None:\n",
        "            # print(mask.shape)\n",
        "            scale_qk += (mask * -1e9)\n",
        "\n",
        "        attention_weight = tf.nn.softmax(scale_qk)\n",
        "\n",
        "        output = tf.matmul(attention_weight,v)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def splite_heads(self,x):\n",
        "        self.batch_size = tf.shape(x)[0]\n",
        "        x = tf.reshape(x,(self.batch_size,-1,self.head_num,self.sub_hidden)) #shape = (batch_size,seq,num_heads,sub_hidden)\n",
        "        return tf.transpose(x,perm=[0,2,1,3]) # 将head和seq交换位置，使seq-sub_hidden成为被计算的矩阵\n",
        "\n",
        "    def call(self,q,k,v,mask):\n",
        "        q = self.dense_q(q)\n",
        "        k = self.dense_k(k)\n",
        "        v = self.dense_v(v)\n",
        "\n",
        "        q = self.splite_heads(q)\n",
        "        k = self.splite_heads(k)\n",
        "        v = self.splite_heads(v)\n",
        "\n",
        "        selfattention = self.scale_dot_product(q,k,v,mask=mask)\n",
        "        selfattention = tf.transpose(selfattention,perm=[0,2,1,3])\n",
        "\n",
        "        concated = tf.reshape(selfattention,(self.batch_size,-1,self.hidden_size))\n",
        "\n",
        "        output = self.dense_output(concated)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69rlWIjsyBkD"
      },
      "outputs": [],
      "source": [
        "# temp_mha = MultiHeadAttention(head_num=8,hidden_size=512)\n",
        "# y = tf.random.uniform((1, 60, 512))\n",
        "# out = temp_mha(y,y,y,mask=None)\n",
        "# out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLjHAVQUyhWD"
      },
      "outputs": [],
      "source": [
        "# @tf.keras.utils.register_keras_serializable()\n",
        "class Encoder(layers.Layer):\n",
        "    def __init__(self,head_num,hidden_size):\n",
        "        super().__init__()\n",
        "        self.head_num = head_num\n",
        "        self.hidden_size = hidden_size\n",
        "        self.MHA = MultiHeadAttention(head_num,hidden_size)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.ffn_1 = layers.Dense(self.hidden_size*4,activation='relu')\n",
        "        self.ffn_2 = layers.Dense(self.hidden_size)\n",
        "\n",
        "    def call(self,x,mask):\n",
        "        res = x\n",
        "        x = self.MHA(q=x,k=x,v=x,mask=mask)\n",
        "        x = tf.add(res,x)\n",
        "        x = self.norm_1(x)\n",
        "        res = x\n",
        "        x = self.ffn_1(x)\n",
        "        x = self.ffn_2(x)\n",
        "        x = tf.add(res,x)\n",
        "        x = self.norm_2(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(layers.Layer):\n",
        "    def __init__(self,head_num,hidden_size):\n",
        "        super().__init__()\n",
        "        self.head_num = head_num\n",
        "        self.hidden_size = hidden_size\n",
        "        self.MHA_1 = MultiHeadAttention(head_num,hidden_size)\n",
        "        self.MHA_2 = MultiHeadAttention(head_num,hidden_size)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.ffn_1 = layers.Dense(self.hidden_size*4,activation='relu')\n",
        "        self.ffn_2 = layers.Dense(self.hidden_size)\n",
        "\n",
        "    def call(self,x,enc_out,look_ahead_mask,padding_mask):\n",
        "        res = x\n",
        "        x = self.MHA_1(x,x,x,mask=look_ahead_mask)\n",
        "        x = self.norm_1(res+x)\n",
        "        \n",
        "        res = x\n",
        "        x = self.MHA_2(q=x,k=enc_out,v=enc_out,mask=padding_mask)\n",
        "        x = self.norm_2(res+x)\n",
        "\n",
        "        res = x\n",
        "        x = self.ffn_1(x)\n",
        "        x = self.ffn_2(x)\n",
        "        x = self.norm_3(res+x)\n",
        "\n",
        "        return x\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y208p9azPCXt"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    def get_angles(pos, i, d_model):\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "        return pos * angle_rates\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],np.arange(d_model)[np.newaxis, :],d_model)\n",
        "\n",
        "    # 将 sin 应用于数组中的偶数索引（indices）；2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # 将 cos 应用于数组中的奇数索引；2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZYYJvXpjX6l"
      },
      "outputs": [],
      "source": [
        "class Transformer(models.Model):\n",
        "    def __init__(self,head_num,hidden_size,layer_num):\n",
        "        super().__init__()\n",
        "        self.head_num = head_num\n",
        "        self.hidden_size = hidden_size\n",
        "        self.layer_num = layer_num\n",
        "        self.vocab_size = 9000\n",
        "\n",
        "        self.enc_layers = [Encoder(head_num=head_num,hidden_size=hidden_size) for _ in range(layer_num)]\n",
        "        self.dec_layers = [Decoder(head_num=head_num,hidden_size=hidden_size) for _ in range(layer_num)]\n",
        "\n",
        "        self.inp_emb = layers.Embedding(self.vocab_size,hidden_size)\n",
        "        self.out_emb = layers.Embedding(self.vocab_size,hidden_size)\n",
        "\n",
        "        self.position = positional_encoding(self.vocab_size,hidden_size)\n",
        "\n",
        "        self.dense = layers.Dense(self.vocab_size,activation='softmax')\n",
        "    def call(self,inputs):\n",
        "        enc_inp,dec_inp,enc_padding_mask,dec_padding_mask,look_ahead_mask = inputs\n",
        "        \n",
        "        enc_seq_len = tf.shape(enc_inp)[1]\n",
        "        dec_seq_len = tf.shape(dec_inp)[1]\n",
        "\n",
        "        ## encoder layers\n",
        "        x_e = self.inp_emb(enc_inp)\n",
        "        x_e += self.position[:,:enc_seq_len,:]\n",
        "        for i in range(self.layer_num):\n",
        "            x_e = self.enc_layers[i](x_e,enc_padding_mask)\n",
        "        \n",
        "        ## decoder layers\n",
        "        x_d = self.out_emb(dec_inp)\n",
        "        x_d += self.position[:,:dec_seq_len,:]\n",
        "        for i in range(self.layer_num):\n",
        "            x_d = self.dec_layers[i](x_d,x_e,look_ahead_mask,dec_padding_mask)\n",
        "        \n",
        "        ## output layer\n",
        "        output = self.dense(x_d)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvz_09M2douN"
      },
      "outputs": [],
      "source": [
        "#test\n",
        "tmp = Transformer(head_num=8,hidden_size=512,layer_num=4)\n",
        "# temp_input = tf.random.uniform((64, 26))\n",
        "temp_target = tf.random.uniform((64, 26))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKmx3Ae5d4Bu"
      },
      "outputs": [],
      "source": [
        "# x = tmp((temp_input,temp_target,None,None,None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5F_FaynfHJe"
      },
      "outputs": [],
      "source": [
        "# x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgwKJ79Q6h38"
      },
      "outputs": [],
      "source": [
        "loss_obj = keras.losses.SparseCategoricalCrossentropy(from_logits=False,reduction='none')\n",
        "def custom_loss(real,pred):\n",
        "\n",
        "    \n",
        "    _loss = loss_obj(real,pred)\n",
        "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
        "    mask = tf.cast(mask,dtype=_loss.dtype)\n",
        "    # print(mask)\n",
        "    _loss *= mask\n",
        "    _loss = tf.reduce_mean(_loss)\n",
        "    return _loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjadLpDk9Dnd"
      },
      "outputs": [],
      "source": [
        "# custom_loss(temp_target,x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejjfk_xasyOn"
      },
      "outputs": [],
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaguXuzCs1bJ"
      },
      "outputs": [],
      "source": [
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
        "\n",
        "tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP1-FTgytuqp",
        "outputId": "e4859900-2ed9-4c93-b620-5a894dbc0d69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized string is [3, 57, 13, 2799, 7877]\n",
            "The original string: the people is awesome.\n",
            "3 ----> the \n",
            "57 ----> people \n",
            "13 ----> is \n",
            "2799 ----> awesome\n",
            "7877 ----> .\n"
          ]
        }
      ],
      "source": [
        "## !! 大小写敏感，应在之前执行全小写转换\n",
        "sample_string = 'the people is awesome.'\n",
        "\n",
        "tokenized_string = tokenizer_en.encode(sample_string)\n",
        "print ('Tokenized string is {}'.format(tokenized_string))\n",
        "\n",
        "original_string = tokenizer_en.decode(tokenized_string)\n",
        "print ('The original string: {}'.format(original_string))\n",
        "\n",
        "assert original_string == sample_string\n",
        "\n",
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6PklhSttviK"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ9sWnpBtwkt"
      },
      "outputs": [],
      "source": [
        "def encode(lang1, lang2):\n",
        "  ## 字典长度作为起始，字典长度+1作为结束的编码。其编码不与字符编码冲突\n",
        "  lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
        "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
        "\n",
        "  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
        "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
        "\n",
        "  return lang1, lang2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wSxeljItyBe"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 600\n",
        "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
        "  return tf.logical_and(tf.size(x) <= max_length,\n",
        "                        tf.size(y) <= max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Gq1KkyBtywJ"
      },
      "outputs": [],
      "source": [
        "def tf_encode(pt, en):\n",
        "  ## !! 为什么要包装成两次函数调用\n",
        "  result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
        "  result_pt.set_shape([None])\n",
        "  result_en.set_shape([None])\n",
        "\n",
        "  return result_pt, result_en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3kEmGDot1RO"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_examples.map(tf_encode)\n",
        "# train_dataset = train_dataset.filter(filter_max_length)\n",
        "# 将数据集缓存到内存中以加快读取速度。\n",
        "# train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE) #乱序，以及batch化\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "val_dataset = val_examples.map(tf_encode)\n",
        "val_dataset = val_dataset.padded_batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijSOJdomr_O2"
      },
      "outputs": [],
      "source": [
        "class Mask():\n",
        "    def create_padding_mask(self,seq):\n",
        "        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "        # 添加额外的维度来将填充加到\n",
        "        # 注意力对数（logits）。\n",
        "        return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "    \n",
        "    def create_look_ahead_mask(self,size):\n",
        "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)  # tf.linalg.band_part(input,numlower,upper) 表明保留多少级次下对角线与次上对角线。-1代表全保留。因此全一矩阵保留下界再取反，则刚好为前瞻遮挡形状\n",
        "        # print(tf.linalg.band_part(tf.ones((size, size)), -1, 0))\n",
        "        return mask  # (seq_len, seq_len)\n",
        "\n",
        "\n",
        "    def create_masks(self,inp,tar):\n",
        "        enc_padding_mask = self.create_padding_mask(inp)\n",
        "\n",
        "        # 在解码器的第二个注意力模块使用。\n",
        "        # 该填充遮挡用于遮挡编码器的输出。\n",
        "        dec_padding_mask = self.create_padding_mask(inp)\n",
        "\n",
        "        # 在解码器的第一个注意力模块使用。\n",
        "        # 用于填充（pad）和遮挡（mask）解码器获取到的输入的后续标记（future tokens）。\n",
        "        look_ahead_mask = self.create_look_ahead_mask(tf.shape(tar)[1])\n",
        "        dec_target_padding_mask = self.create_padding_mask(tar)\n",
        "        combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "        # print('enc_padding_mask{}\\n combined_mask{}\\n, dec_padding_mask{}'.format(enc_padding_mask.shape, combined_mask.shape, dec_padding_mask.shape))\n",
        "        return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "    def __call__(self,inp,tar):\n",
        "        return self.create_masks(inp,tar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og8xsP22u_xS"
      },
      "outputs": [],
      "source": [
        "def data_generrator(dataset):\n",
        "    mask = Mask()\n",
        "    for pt,en in dataset.repeat():\n",
        "        inp = pt\n",
        "        tar_inp = en[:,:-1]\n",
        "        tar_rel = en[:,1:]\n",
        "        enc_padding_mask,combined_mask,dec_padding_mask = mask(inp,tar_inp)\n",
        "        yield ([inp,tar_inp,enc_padding_mask,dec_padding_mask,combined_mask],tar_rel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMxNZkA-ZQok"
      },
      "outputs": [],
      "source": [
        "gen = data_generrator(train_dataset)\n",
        "val_gen = data_generrator(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CH3pti9iaiZX"
      },
      "outputs": [],
      "source": [
        "per_epoch_step = len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFE_yW8smATS",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "learning_rate = CustomSchedule(512)\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWp9kWb4petN",
        "outputId": "4bd84e61-f9bc-441a-f564-b44c270302e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=1.7469279e-06>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "learning_rate(10.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmekWon0RJFu"
      },
      "outputs": [],
      "source": [
        "model = Transformer(head_num=8,hidden_size=512,layer_num=5)\n",
        "model.compile(loss=custom_loss,optimizer='adam',metrics='acc')\n",
        "# model.fit((gen),epochs=20,steps_per_epoch=per_epoch_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuZcGyWdpetN",
        "outputId": "3e24dfad-3add-45fc-e17f-2a184d024f73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1619/1619 [==============================] - 767s 463ms/step - loss: 2.2213 - acc: 0.0140\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1c878d1ac70>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit((gen),epochs=1,steps_per_epoch=per_epoch_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NH4kagCspetN",
        "outputId": "182b7462-5c1c-46a5-8217-812bcd5553aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_attention_12_layer_call_fn, multi_head_attention_12_layer_call_and_return_conditional_losses, layer_normalization_20_layer_call_fn, layer_normalization_20_layer_call_and_return_conditional_losses, layer_normalization_21_layer_call_fn while saving (showing 5 of 600). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./transfromer/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./transfromer/assets\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x00000287A0062670> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x000002897737AA60> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x000002897957A310> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x000002897721C280> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x00000287DD543100> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x000002897954BB20> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x000002897A69C4F0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x000002897FD54070> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x000002897FD54B80> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x000002897FD47E20> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x000002897FD77C70> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x000002897FD71B50> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x00000289795759A0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x000002897954C880> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x000002897954F6D0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ],
      "source": [
        "model.save('./transfromer/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngW-Q72opetO",
        "outputId": "b79bafbb-ac97-4023-cd2e-252c5330f11b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"transformer_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_4 (Encoder)         multiple                  3152384   \n",
            "                                                                 \n",
            " encoder_5 (Encoder)         multiple                  3152384   \n",
            "                                                                 \n",
            " encoder_6 (Encoder)         multiple                  3152384   \n",
            "                                                                 \n",
            " encoder_7 (Encoder)         multiple                  3152384   \n",
            "                                                                 \n",
            " encoder_8 (Encoder)         multiple                  3152384   \n",
            "                                                                 \n",
            " decoder_4 (Decoder)         multiple                  4204032   \n",
            "                                                                 \n",
            " decoder_5 (Decoder)         multiple                  4204032   \n",
            "                                                                 \n",
            " decoder_6 (Decoder)         multiple                  4204032   \n",
            "                                                                 \n",
            " decoder_7 (Decoder)         multiple                  4204032   \n",
            "                                                                 \n",
            " decoder_8 (Decoder)         multiple                  4204032   \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     multiple                  4608000   \n",
            "                                                                 \n",
            " embedding_3 (Embedding)     multiple                  4608000   \n",
            "                                                                 \n",
            " dense_145 (Dense)           multiple                  4617000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 50,615,080\n",
            "Trainable params: 50,615,080\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfbMJ3H6petO",
        "outputId": "332a9d10-316b-450d-d0d7-ce4a63d637fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1c6fb760cd0>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weight_path = \"transformer_2.weight\"\n",
        "model.load_weights(weight_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nze0D2EBpetO",
        "outputId": "3056efbc-1b6a-4d84-ec32-5da56c09da2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1619/1619 [==============================] - 751s 464ms/step - loss: 0.9202 - acc: 0.1242\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1c892c05580>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model.compile(loss=custom_loss,optimizer=opt,metrics='acc')\n",
        "model.fit((gen),epochs=1,steps_per_epoch=per_epoch_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi1bj2zIpetO"
      },
      "outputs": [],
      "source": [
        "model.compile(\"rmsprop\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t2135xOpetO"
      },
      "outputs": [],
      "source": [
        "model.fit((gen),epochs=50,steps_per_epoch=per_epoch_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLLqE-vIpetO",
        "outputId": "2f39b0ac-416f-4a9c-99bd-3871a8d04de5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1619/1619 [==============================] - 949s 577ms/step - loss: 1.1952 - accuracy: 0.7886\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1c87e20dc40>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit((gen),epochs=1,steps_per_epoch=per_epoch_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzw8WxdspetP"
      },
      "outputs": [],
      "source": [
        "filepath = \"./transformer_2.weight\"\n",
        "cp = keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBc-3EP8petP",
        "outputId": "bd1875bb-c591-47b3-d705-2bdbf5da2a92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1619/1619 [==============================] - ETA: 0s - loss: 1.1287 - acc: 0.1040\n",
            "Epoch 00001: loss did not improve from 1.10299\n",
            "1619/1619 [==============================] - 765s 473ms/step - loss: 1.1287 - acc: 0.1040\n",
            "Epoch 2/50\n",
            "1619/1619 [==============================] - ETA: 0s - loss: 1.0989 - acc: 0.1057\n",
            "Epoch 00002: loss improved from 1.10299 to 1.09892, saving model to .\\transformer_2.weight\n",
            "1619/1619 [==============================] - 769s 475ms/step - loss: 1.0989 - acc: 0.1057\n",
            "Epoch 3/50\n",
            "1619/1619 [==============================] - ETA: 0s - loss: 1.1049 - acc: 0.1057\n",
            "Epoch 00003: loss did not improve from 1.09892\n",
            "1619/1619 [==============================] - 765s 472ms/step - loss: 1.1049 - acc: 0.1057\n",
            "Epoch 4/50\n",
            "1619/1619 [==============================] - ETA: 0s - loss: 1.1032 - acc: 0.1051\n",
            "Epoch 00004: loss did not improve from 1.09892\n",
            "1619/1619 [==============================] - 768s 474ms/step - loss: 1.1032 - acc: 0.1051\n",
            "Epoch 5/50\n",
            "1619/1619 [==============================] - ETA: 0s - loss: 1.1146 - acc: 0.1056\n",
            "Epoch 00005: loss did not improve from 1.09892\n",
            "1619/1619 [==============================] - 765s 473ms/step - loss: 1.1146 - acc: 0.1056\n",
            "Epoch 6/50\n",
            "1619/1619 [==============================] - ETA: 0s - loss: 1.1034 - acc: 0.1063\n",
            "Epoch 00006: loss did not improve from 1.09892\n",
            "1619/1619 [==============================] - 758s 468ms/step - loss: 1.1034 - acc: 0.1063\n",
            "Epoch 7/50\n",
            "1619/1619 [==============================] - ETA: 0s - loss: 1.1004 - acc: 0.1067\n",
            "Epoch 00007: loss did not improve from 1.09892\n",
            "1619/1619 [==============================] - 767s 474ms/step - loss: 1.1004 - acc: 0.1067\n",
            "Epoch 8/50\n",
            "1619/1619 [==============================] - ETA: 0s - loss: 1.0958 - acc: 0.1072\n",
            "Epoch 00008: loss improved from 1.09892 to 1.09584, saving model to .\\transformer_2.weight\n",
            "1619/1619 [==============================] - 770s 476ms/step - loss: 1.0958 - acc: 0.1072\n",
            "Epoch 9/50\n",
            "1619/1619 [==============================] - ETA: 0s - loss: 1.0825 - acc: 0.1068\n",
            "Epoch 00009: loss improved from 1.09584 to 1.08255, saving model to .\\transformer_2.weight\n",
            "1619/1619 [==============================] - 773s 478ms/step - loss: 1.0825 - acc: 0.1068\n",
            "Epoch 10/50\n",
            "1619/1619 [==============================] - ETA: 0s - loss: 1.0955 - acc: 0.1078\n",
            "Epoch 00010: loss did not improve from 1.08255\n",
            "1619/1619 [==============================] - 765s 473ms/step - loss: 1.0955 - acc: 0.1078\n",
            "Epoch 11/50\n",
            "1619/1619 [==============================] - ETA: 0s - loss: 1.0859 - acc: 0.1078\n",
            "Epoch 00011: loss did not improve from 1.08255\n",
            "1619/1619 [==============================] - 767s 474ms/step - loss: 1.0859 - acc: 0.1078\n",
            "Epoch 12/50\n",
            "1619/1619 [==============================] - ETA: 0s - loss: 1.0970 - acc: 0.1068\n",
            "Epoch 00012: loss did not improve from 1.08255\n",
            "1619/1619 [==============================] - 767s 474ms/step - loss: 1.0970 - acc: 0.1068\n",
            "Epoch 13/50\n",
            "1619/1619 [==============================] - ETA: 0s - loss: 1.0938 - acc: 0.1071\n",
            "Epoch 00013: loss did not improve from 1.08255\n",
            "1619/1619 [==============================] - 766s 473ms/step - loss: 1.0938 - acc: 0.1071\n",
            "Epoch 14/50\n",
            " 589/1619 [=========>....................] - ETA: 7:52 - loss: 1.0742 - acc: 0.1089"
          ]
        }
      ],
      "source": [
        "model.fit((gen),epochs=50,steps_per_epoch=per_epoch_step,callbacks=[cp])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuAVT0tEpetP",
        "outputId": "6346743f-e3c0-456e-a292-229e5d856614"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1111\n"
          ]
        }
      ],
      "source": [
        "print(1111)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4pW9Wc7petP"
      },
      "outputs": [],
      "source": [
        "model.fit((gen),epochs=50,steps_per_epoch=per_epoch_step,callbacks=[cp])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ3pI2DspetP"
      },
      "outputs": [],
      "source": [
        "model.fit((gen),epochs=500,steps_per_epoch=per_epoch_step,callbacks=[cp])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dym7FDeGpetP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "transformer_2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}